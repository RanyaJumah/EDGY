{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQqgdtEQdIzg"
      },
      "source": [
        "Steps:\n",
        "1. Read TensorFlow Dataset data as numpy\n",
        "2. Convert audio to float and resample\n",
        "3. Convert audio to embeddings\n",
        "4. Train and eval sklearn model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7zQrGJXTEG5"
      },
      "source": [
        "tfds_dataset_name = 'savee'  #@param\n",
        "REQUIRED_SAMPLE_RATE_ = 16000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhjeHdWpd0dW"
      },
      "source": [
        "# Read the data into numpy arrays.\n",
        "import collections\n",
        "SingleSplit = collections.namedtuple(\n",
        "    'SingleSplit', ['audio', 'labels', 'speaker_id'])\n",
        "Data = collections.namedtuple(\n",
        "    'Data', ['train', 'validation', 'test'])\n",
        "\n",
        "import tensorflow.compat.v2 as tf\n",
        "tf.enable_v2_behavior()\n",
        "assert tf.executing_eagerly()\n",
        "import tensorflow_datasets as tfds\n",
        "def _dat_from_split(split):\n",
        "  np_generator = tfds.as_numpy(tfds.load(tfds_dataset_name, split=split))\n",
        "  dat = [(x['audio'], x['label'], x['speaker_id']) for x in np_generator]\n",
        "  audio, labels, speaker_id = zip(*dat)\n",
        "\n",
        "  import numpy as np\n",
        "  labels = np.array(labels, dtype=np.int16)\n",
        "  speaker_id = np.array(speaker_id)\n",
        "  assert len(audio) == labels.size == speaker_id.size\n",
        "  assert labels.ndim == speaker_id.ndim == 1\n",
        "  print(f'Finished {split}')\n",
        "  return audio, labels, speaker_id\n",
        "\n",
        "all_data = Data(\n",
        "    train=SingleSplit(*_dat_from_split('train')),\n",
        "    validation=SingleSplit(*_dat_from_split('validation')),\n",
        "    test=SingleSplit(*_dat_from_split('test')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8s5nFOJd4TQ"
      },
      "source": [
        "# Make the audio floats, and resample the audio if necessary.\n",
        "import collections\n",
        "import librosa\n",
        "import numpy as np\n",
        "FloatData = collections.namedtuple('FloatData', ['train', 'validation', 'test'])\n",
        "\n",
        "sample_rate = tfds.builder(tfds_dataset_name).info.features['audio'].sample_rate\n",
        "def _int_to_float(audio_int16, split_name):\n",
        "  float_audio_16k = []\n",
        "  for i, samples in enumerate(audio_int16):\n",
        "    float_audio = samples.astype(np.float32) / np.iinfo(np.int16).max\n",
        "    if sample_rate != REQUIRED_SAMPLE_RATE_:\n",
        "      float_audio = librosa.core.resample(\n",
        "          float_audio, orig_sr=sample_rate, target_sr=16000, \n",
        "          res_type='kaiser_best')\n",
        "    float_audio_16k.append(float_audio)\n",
        "    if i % 50 == 0:\n",
        "      print(f'Finished resampling {i} / {len(audio_int16)} for {split_name}')\n",
        "  print(f'Finished {split_name}')\n",
        "  return float_audio_16k\n",
        "\n",
        "\n",
        "float_audio_16k = FloatData(\n",
        "    train=_int_to_float(all_data.train.audio, 'train'),\n",
        "    validation=_int_to_float(all_data.validation.audio, 'validation'),\n",
        "    test=_int_to_float(all_data.test.audio, 'test'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwaXIUwId7fh"
      },
      "source": [
        "tfhub_model_name = 'https://tfhub.dev/google/nonsemantic-speech-benchmark/trill-distilled/1'  #@param\n",
        "output_key = 'embedding'  #@param"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXPI0aSjd_iK"
      },
      "source": [
        "# Convert the audio to embeddings. Preaverage the embeddings across time.\n",
        "import tensorflow_hub as hub\n",
        "model = hub.load(tfhub_model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc6E5sEMeBvW"
      },
      "source": [
        "import collections\n",
        "Embeddings = collections.namedtuple(\n",
        "    'Embeddings', ['train', 'validation', 'test'])\n",
        "\n",
        "def _calc_embeddings(cur_float_audio, split_name):\n",
        "  cur_embeddings = []\n",
        "  for i, float_samples in enumerate(cur_float_audio):\n",
        "    tf_out = model(tf.constant(float_samples, tf.float32),\n",
        "                  tf.constant(16000, tf.int32))\n",
        "    embedding_2d = tf_out[output_key]\n",
        "    assert embedding_2d.ndim == 2\n",
        "    embedding_1d = np.mean(embedding_2d, axis=0)\n",
        "    cur_embeddings.append(embedding_1d)\n",
        "    if i % 50 == 0:\n",
        "      print(f'Finished embedding {i} / {len(cur_float_audio)} for {split_name}')\n",
        "  print(f'Finished {split_name}')\n",
        "  cur_embeddings = np.array(cur_embeddings, dtype=np.float32)\n",
        "  return cur_embeddings\n",
        "\n",
        "embeddings = Embeddings(\n",
        "    train=_calc_embeddings(float_audio_16k.train, 'train'),\n",
        "    validation=_calc_embeddings(float_audio_16k.validation, 'validation'),\n",
        "    test=_calc_embeddings(float_audio_16k.test, 'test'))\n",
        "assert embeddings.train.shape[1] == embeddings.validation.shape[1] == embeddings.test.shape[1]\n",
        "assert embeddings.train.shape[0] == all_data.train.labels.shape[0] == all_data.train.speaker_id.shape[0]\n",
        "assert embeddings.validation.shape[0] == all_data.validation.labels.shape[0] == all_data.validation.speaker_id.shape[0]\n",
        "assert embeddings.test.shape[0] == all_data.test.labels.shape[0] == all_data.test.speaker_id.shape[0]\n",
        "assert not np.isnan(embeddings.train).any()\n",
        "assert not np.isnan(embeddings.validation).any()\n",
        "assert not np.isnan(embeddings.test).any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJOycvr8eFn5"
      },
      "source": [
        "model_name = 'LogisticRegression_balanced'  #@param"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4f8LtMkeIR-"
      },
      "source": [
        "from sklearn import linear_model\n",
        "\n",
        "def get_sklearn_model(model_name):\n",
        "  return {\n",
        "      'LogisticRegression': lambda: linear_model.LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial'),\n",
        "      'LogisticRegression_balanced': lambda: linear_model.LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial', class_weight='balanced'),\n",
        "  }[model_name]()\n",
        "\n",
        "def _speaker_normalization(embedding_np, speaker_id_np):\n",
        "  \"\"\"Normalize embedding features by per-speaker statistics.\"\"\"\n",
        "  all_speaker_ids = np.unique(speaker_id_np)\n",
        "  for speaker in all_speaker_ids:\n",
        "    cur_i = speaker_id_np == speaker\n",
        "    embedding_np[cur_i] -= embedding_np[cur_i].mean(axis=0)\n",
        "    stds = embedding_np[cur_i].std(axis=0)\n",
        "    stds[stds == 0] = 1\n",
        "    embedding_np[cur_i] /= stds\n",
        "\n",
        "  return embedding_np\n",
        "\n",
        "# Train models.\n",
        "d = get_sklearn_model(model_name)\n",
        "normalized_train = _speaker_normalization(\n",
        "    embeddings.train, all_data.train.speaker_id)\n",
        "d.fit(normalized_train, all_data.train.labels)\n",
        "\n",
        "# Eval.\n",
        "normalized_validation = _speaker_normalization(\n",
        "    embeddings.validation, all_data.validation.speaker_id)\n",
        "eval_score = d.score(normalized_validation, all_data.validation.labels)\n",
        "print(f'{model_name} eval score: {eval_score}')\n",
        "\n",
        "# Test.\n",
        "normalized_test = _speaker_normalization(\n",
        "    embeddings.test, all_data.test.speaker_id)\n",
        "test_score = d.score(normalized_test, all_data.test.labels)\n",
        "print(f'{model_name} test score: {test_score}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}