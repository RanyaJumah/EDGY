
# EDGY
```
We evaluate EDGY's on-device performance, and explore optimization techniques, including model pruning and quantization, to enable private, accurate and efficient representation learning on resource-constrained devices.
```
## Introduction
Inference efficiency is a significant challenge when deploying DL models at the edge given restrictions on processing, memory, and in some cases power consumption. To address this challenge, we focus on a variety of techniques that involve reducing model parameters with pruning and/or reducing representational precision with quantization to support efficient inference at the edge. 

We elaborate in the following:
*(1) Low-precision Training.
*(2) Pruning and Sparsity.
*(3) Quantization.
